{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "hn = pd.read_csv('data/hacker_news.csv')\n",
    "titles = hn['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we learned to work with basic string methods, we used the str.replace() method to replace simple substrings. We can achieve the same with regular expressions using the re.sub() function. The basic syntax for re.sub() is:\n",
    "\n",
    "re.sub(pattern, repl, string, flags=0)\n",
    "The repl parameter is the text that you would like to substitute for the match. Let's look at a simple example where we replace all capital letters in a string with dashes:\n",
    "<pre>\n",
    "string = \"aBcDEfGHIj\"\n",
    "\n",
    "print(re.sub(r\"[A-Z]\", \"-\", string))\n",
    "string = \"aBcDEfGHIj\"\n",
    "​\n",
    "print(re.sub(r\"[A-Z]\", \"-\", string))\n",
    "a-c--f---j\n",
    "</pre>\n",
    "When working in pandas, we can use the Series.str.replace() method, which uses nearly identical syntax:\n",
    "\n",
    "Series.str.replace(pat, repl, flags=0)\n",
    "Earlier, we discovered that there were multiple different capitalizations for SQL in our dataset. Let's look at how we could make these uniform with the Series.str.replace() method and a regular expression:\n",
    "<pre>\n",
    "sql_variations = pd.Series([\"SQL\", \"Sql\", \"sql\"])\n",
    "​\n",
    "sql_uniform = sql_variations.str.replace(r\"sql\", \"SQL\", flags=re.I)\n",
    "print(sql_uniform)\n",
    "0    SQL\n",
    "1    SQL\n",
    "2    SQL\n",
    "dtype: object\n",
    "</pre>\n",
    "We have provided email_variations, a pandas Series containing all the variations of \"email\" in the dataset.\n",
    "\n",
    "1. Use a regular expression to replace each of the matches in email_variations with \"email\" and assign the result to email_uniform.\n",
    "    - You may need to iterate several times when writing your regular expression in order to match every item.\n",
    "2. Use the same syntax to replace all mentions of email in titles with \"email\". Assign the result to titles_clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_variations = pd.Series(['email', 'Email', 'e Mail',\n",
    "                        'e mail', 'E-mail', 'e-mail',\n",
    "                        'eMail', 'E-Mail', 'EMAIL'])\n",
    "pattern = r'e.{0,1}mail'\n",
    "email_uniform = email_variations.str.replace(pattern, \"email\", flags=re.I)\n",
    "titles_clean = titles.str.replace(pattern,'email',flags=re.I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the final three screens in this mission, we'll extract components of URLs from our dataset. As a reminder, most stories on Hacker News contain a link to an external resource.\n",
    "\n",
    "The task we will be performing first is extracting the different components of the URLs in order to analyze them. On this screen, we'll start by extracting just the domains. Below is a list of some of the URLs in the dataset, with the domains highlighted in color, so you can see the part of the string we want to capture.\n",
    "\n",
    "<img src='images/url_examples_1.svg' />\n",
    "\n",
    "The domain of each URL excludes the protocol (e.g. https://) and the page path (e.g. /Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429).\n",
    "\n",
    "There are several ways that you could use regular expressions to extract the domain, but we suggest the following technique:\n",
    "\n",
    "1. Using a series of characters that will match the protocol.\n",
    "2. Inside a capture group, using a set that will match the character classes used in the domain.\n",
    "3. Because all of the URLs either end with the domain, or continue with page path which starts with / (a character not found in any domains), we don't need to cater for this part of the URL in our regular expression.\n",
    "4. Once you have extracted the domains, you will be building a frequency table so we can determine the most popular domains. There are over 7,000 unique domains in our dataset, so to make the frequency table easier to analyze, we'll look at only the top 20 domains.\n",
    "\n",
    "\n",
    "1. Write a regular expression to extract the domains from test_urls and assign the result to test_urls_clean. We suggest the following technique:\n",
    "    - Using a series of characters that will match the protocol.\n",
    "    - Inside a capture group, using a set that will match the character classes used in the domain.\n",
    "    - Because all of the URLs either end with the domain, or continue with page path which starts with / (a character not found in any domains), we don't need to cater for this part of the URL in our regular expression.\n",
    "2. Use the same regular expression to extract the domains from the url column of the hn dataframe. Assign the result to domains.\n",
    "3. Use Series.value_counts() to build a frequency table of the domains in domains, limiting the frequency table to just to the top 20. Assign the result to top_domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_urls = pd.Series([\n",
    " 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429',\n",
    " 'http://www.interactivedynamicvideo.com/',\n",
    " 'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0',\n",
    " 'http://evonomics.com/advertising-cannot-maintain-internet-heres-solution/',\n",
    " 'HTTPS://github.com/keppel/pinn',\n",
    " 'Http://phys.org/news/2015-09-scale-solar-youve.html',\n",
    " 'https://iot.seeed.cc',\n",
    " 'http://www.bfilipek.com/2016/04/custom-deleters-for-c-smart-pointers.html',\n",
    " 'http://beta.crowdfireapp.com/?beta=agnipath',\n",
    " 'https://www.valid.ly?param'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "github.com                1008\n",
       "medium.com                 825\n",
       "www.nytimes.com            525\n",
       "www.theguardian.com        248\n",
       "techcrunch.com             245\n",
       "www.youtube.com            213\n",
       "www.bloomberg.com          193\n",
       "arstechnica.com            191\n",
       "www.washingtonpost.com     190\n",
       "www.wsj.com                138\n",
       "www.theatlantic.com        137\n",
       "www.bbc.com                134\n",
       "www.wired.com              114\n",
       "www.theverge.com           112\n",
       "www.bbc.co.uk              108\n",
       "en.wikipedia.org           100\n",
       "twitter.com                 93\n",
       "qz.com                      85\n",
       "motherboard.vice.com        82\n",
       "www.newyorker.com           81\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'https?\\:\\/\\/([\\w\\.]+)'\n",
    "test_urls_clean = test_urls.str.extract(pattern, flags=re.I)\n",
    "domains = hn['url'].str.extract(pattern, flags=re.I)\n",
    "top_domains = domains.iloc[:,0].value_counts().head(20)\n",
    "top_domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having extracted just the domains from the URLs, on this final screen we'll extract each of the three component parts of the URLs:\n",
    "\n",
    "Protocol\n",
    "Domain\n",
    "Page path\n",
    "<img src='images/url_examples_2.svg' />\n",
    "In order to do this, we'll create a regular expression with multiple capture groups. Multiple capture groups in regular expressions are defined the same way as single capture groups — using pairs of parentheses.\n",
    "\n",
    "Let's look at how this works using the first few values from the created_at column in our dataset:\n",
    "<pre>\n",
    "created_at = hn['created_at'].head()\n",
    "print(created_at)\n",
    "0     8/4/2016 11:52\n",
    "1    1/26/2016 19:30\n",
    "2    6/23/2016 22:20\n",
    "3     6/17/2016 0:01\n",
    "4     9/30/2015 4:12\n",
    "Name: created_at, dtype: object\n",
    "</pre>\n",
    "\n",
    "We'll use capture groups to extract these dates and times into two columns:\n",
    "<pre>\n",
    "8/4/2016\t11:52\n",
    "1/26/2016\t19:30\n",
    "6/23/2016\t22:20\n",
    "6/17/2016\t0:01\n",
    "9/30/2015\t4:12\n",
    "</pre>\n",
    "<img src='images/multiple_capture_groups.svg' />\n",
    "\n",
    "In order to do this we can write the following regular expression:\n",
    "\n",
    "\n",
    "Notice how we put a space character between the capture groups, which matches the space character in the original strings.\n",
    "\n",
    "Let's look at the result of using this regex pattern with Series.str.extract():\n",
    "<pre>\n",
    "pattern = r\"(.+)\\s(.+)\"\n",
    "dates_times = created_at.str.extract(pattern)\n",
    "print(dates_times)\n",
    "_          0      1\n",
    "0   8/4/2016  11:52\n",
    "1  1/26/2016  19:30\n",
    "2  6/23/2016  22:20\n",
    "3  6/17/2016   0:01\n",
    "4  9/30/2015   4:12\n",
    "</pre>\n",
    "\n",
    "The result is a dataframe with each of our capture groups defining a column of data.\n",
    "\n",
    "Now let's write a regular expression that will extract the URL components into individual columns of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_urls = pd.Series([\n",
    " 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429',\n",
    " 'http://www.interactivedynamicvideo.com/',\n",
    " 'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0',\n",
    " 'http://evonomics.com/advertising-cannot-maintain-internet-heres-solution/',\n",
    " 'HTTPS://github.com/keppel/pinn',\n",
    " 'Http://phys.org/news/2015-09-scale-solar-youve.html',\n",
    " 'https://iot.seeed.cc',\n",
    " 'http://www.bfilipek.com/2016/04/custom-deleters-for-c-smart-pointers.html',\n",
    " 'http://beta.crowdfireapp.com/?beta=agnipath',\n",
    " 'https://www.valid.ly?param'\n",
    "])\n",
    "pattern = r'https?\\:\\/\\/([\\w\\.]+)'\n",
    "test_urls_clean = test_urls.str.extract(pattern, flags=re.I)\n",
    "domains = hn['url'].str.extract(pattern, flags=re.I)\n",
    "top_domains = domains.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `test_urls` is available from the previous screen\n",
    "pattern = r\"(.+)://([\\w\\.]+)/?(.*)\"\n",
    "test_url_parts = test_urls.str.extract(pattern, flags=re.I)\n",
    "url_parts = hn['url'].str.extract(pattern, flags=re.I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided the regex pattern from the previous screen's solution.\n",
    "\n",
    "1. Uncomment the regular expression pattern. Add names to each capture group:\n",
    "    - The first capture group should be called protocol.\n",
    "    - The second capture group should be called domain.\n",
    "    - The third capture group should be called path.\n",
    "2. Use the regular expression pattern to extract three named columns of url components from the url column of the hn dataframe. Assign the result to url_parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We have provided the regex pattern from the previous screen's solution.\n",
    "\n",
    "Uncomment the regular expression pattern. Add names to each capture group:\n",
    "The first capture group should be called protocol.\n",
    "The second capture group should be called domain.\n",
    "The third capture group should be called path.\n",
    "Use the regular expression pattern to extract three named columns of url components from the url column of the hn dataframe. Assign the result to url_parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
