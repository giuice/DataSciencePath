{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark and Python \n",
    "The core data structure in Spark is a resilient distributed data set (RDD). As the name suggests, an RDD is Spark's representation of a data set that's distributed across the RAM, or memory, of a cluster of many machines. An RDD object is essentially a collection of elements we can use to hold lists of tuples, dictionaries, lists, etc. Similar to a pandas DataFrame, we can load a data set into an RDD, and then run any of the methods accesible to that object.\n",
    "\n",
    "PySpark\n",
    "\n",
    "While the Spark toolkit is in Scala, a language that compiles down to bytecode for the JVM, the open source community has developed a wonderful toolkit called <a href='https://spark.apache.org/docs/0.9.0/python-programming-guide.html'>PySpark</a> that allows us to interface with RDDs in Python. Thanks to a library called <a href='https://github.com/bartdag/py4j'>Py4J</a>, Python can interface with Java objects (in our case RDDs). Py4J is also one of the tools that makes PySpark work.\n",
    "\n",
    "In this mission, we'll work with a data set containing the names of all of the guests who have appeared on The Daily Show.\n",
    "\n",
    "To start off, we'll load the data set into an RDD. We're using the TSV version of FiveThirtyEight's data set. TSV files use a tab character (\"\\t\") as the delimiter, instead of the comma (\",\") that CSV files use.The core data structure in Spark is a resilient distributed data set (RDD). As the name suggests, an RDD is Spark's representation of a data set that's distributed across the RAM, or memory, of a cluster of many machines. An RDD object is essentially a collection of elements we can use to hold lists of tuples, dictionaries, lists, etc. Similar to a pandas DataFrame, we can load a data set into an RDD, and then run any of the methods accesible to that object.\n",
    "\n",
    "PySpark\n",
    "\n",
    "While the Spark toolkit is in Scala, a language that compiles down to bytecode for the JVM, the open source community has developed a wonderful toolkit called PySpark that allows us to interface with RDDs in Python. Thanks to a library called Py4J, Python can interface with Java objects (in our case RDDs). Py4J is also one of the tools that makes PySpark work.\n",
    "\n",
    "In this mission, we'll work with a data set containing the names of all of the guests who have appeared on The Daily Show.\n",
    "\n",
    "To start off, we'll load the data set into an RDD. We're using the TSV version of FiveThirtyEight's data set. TSV files use a tab character (\"\\t\") as the delimiter, instead of the comma (\",\") that CSV files use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YEAR\\tGoogleKnowlege_Occupation\\tShow\\tGroup\\tRaw_Guest_List',\n",
       " '1999\\tactor\\t1/11/99\\tActing\\tMichael J. Fox',\n",
       " '1999\\tComedian\\t1/12/99\\tComedy\\tSandra Bernhard',\n",
       " '1999\\ttelevision actress\\t1/13/99\\tActing\\tTracey Ullman',\n",
       " '1999\\tfilm actress\\t1/14/99\\tActing\\tGillian Anderson']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = sc.textFile(\"data/daily_show.tsv\")\n",
    "raw_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark, the SparkContext object manages the connection to the clusters, and coordinates the running of processes on those clusters. More specifically, it connects to the cluster managers. The cluster managers control the executors that run the computations. Here's a diagram from the Spark documentation that will help you visualize the architecture:\n",
    "\n",
    "<img src='images/cluster-overview.png' />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>While Spark borrowed heavily from Hadoop's MapReduce pattern, it's still quite different in many ways. If you have experience with Hadoop and traditional MapReduce, you may want to read this great <a href=\"http://blog.cloudera.com/blog/2014/09/how-to-translate-from-mapreduce-to-apache-spark/\" target=\"_blank\">post by Cloudera</a> about the difference between them. Don't worry if you've never worked with MapReduce or Hadoop before; we'll cover the concepts you need to know in this course.</p>\n",
    "<p>The key idea to understand when working with Spark is data <strong>pipelining</strong>. Every operation or calculation in Spark is essentially a series of steps that we can chain together and run in succession to form a <strong>pipeline</strong>. Each step in the <strong>pipeline</strong> returns either a Python value (such as an integer), a Python data structure (such as a dictionary), or an RDD object. We'll start with the <code>map()</code> function.</p>\n",
    "<p><strong>Map()</strong></p>\n",
    "<p>The <code>map(f)</code> function applies the function <code>f</code> to every element in the RDD. Because RDDs are iterable objects (like most Python objects), Spark runs function <code>f</code> on each iteration and returns a new RDD.</p>\n",
    "<p>We'll walk through an example of a <code>map</code> function so you can get a better sense of how it works. If you look carefully, you'll see that <code>raw_data</code> is in a format that's hard to work with. While the elements are currently all <code>strings</code>, we'd like to convert each of them into a <code>list</code> to make the data more manageable. To do this the traditional way, we would:</p>\n",
    "</div>\n",
    "<div><textarea style=\"display: none;\">1. Use a 'for' loop to iterate over the collection\n",
    "2. Split each `string` on the delimiter\n",
    "3. Store the result in a `list`</textarea>\n",
    "<div>\n",
    "<div style=\"overflow: hidden; position: relative; width: 3px; height: 0px; top: 7.5px; left: 4px;\"> </div>\n",
    "<div> </div>\n",
    "<div> </div>\n",
    "<div> </div>\n",
    "<div> </div>\n",
    "<div tabindex=\"-1\">\n",
    "<div style=\"margin-left: 0px; margin-bottom: -11px; border-right-width: 19px; min-height: 73px; padding-right: 0px; padding-bottom: 0px;\">\n",
    "<div style=\"position: relative; top: 0px;\">\n",
    "<div>\n",
    "<div style=\"position: relative; outline: none;\">\n",
    "<div>\n",
    "<pre>xxxxxxxxxx</pre>\n",
    "</div>\n",
    "<div> </div>\n",
    "<div style=\"position: relative; z-index: 1;\"> </div>\n",
    "<div>\n",
    "<div style=\"left: 4px; top: 0px; height: 18px;\"> </div>\n",
    "</div>\n",
    "<div>\n",
    "<pre>1. Use a 'for' loop to iterate over the collection</pre>\n",
    "<pre>2. Split each `string` on the delimiter</pre>\n",
    "<pre>3. Store the result in a `list`</pre>\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"position: absolute; height: 19px; width: 1px; border-bottom: 0px solid transparent; top: 73px;\"> </div>\n",
    "<div style=\"display: none; height: 92px;\"> </div>\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "<div>\n",
    "<p>Let's see how we can use <code>map</code> to do this with Spark instead.</p>\n",
    "<p>In the code cell:</p>\n",
    "</div>\n",
    "<div><textarea style=\"display: none;\">1. Call the RDD function `map()` to specify we want to apply the logic in the parentheses to every line in our data set.\n",
    "2. Write a lambda function that splits each line using the tab delimiter (\\t), and assign the resulting RDD to `daily_show`.\n",
    "3. Call the RDD function `take()` on `daily_show` to display the first five elements (or rows) of the resulting RDD.</textarea>\n",
    "<div>\n",
    "<div style=\"overflow: hidden; position: relative; width: 3px; height: 0px; top: 7.5px; left: 4px;\"> </div>\n",
    "\n",
    "<div tabindex=\"-1\">\n",
    "<div style=\"margin-left: 0px; margin-bottom: -11px; border-right-width: 19px; min-height: 127px; padding-right: 0px; padding-bottom: 0px;\">\n",
    "<div style=\"position: relative; top: 0px;\">\n",
    "<div>\n",
    "<div style=\"position: relative; outline: none;\">\n",
    "<div>\n",
    "<pre>xxxxxxxxxx</pre>\n",
    "</div>\n",
    "<div> </div>\n",
    "<div style=\"position: relative; z-index: 1;\"> </div>\n",
    "<div>\n",
    "<div style=\"left: 4px; top: 0px; height: 18px;\"> </div>\n",
    "</div>\n",
    "<div>\n",
    "<pre>1. Call the RDD function `map()` to specify we want to apply the logic in the parentheses to every line in our data set.</pre>\n",
    "<pre>2. Write a lambda function that splits each line using the tab delimiter (\\t), and assign the resulting RDD to `daily_show`.</pre>\n",
    "<pre>3. Call the RDD function `take()` on `daily_show` to display the first five elements (or rows) of the resulting RDD.</pre>\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"position: absolute; height: 19px; width: 1px; border-bottom: 0px solid transparent; top: 127px;\"> </div>\n",
    "<div style=\"display: none; height: 146px;\"> </div>\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "<div>\n",
    "<p>We call the <code>map(f)</code> function a transformation step. It requires either a named or lambda function <code>f</code>.</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['YEAR', 'GoogleKnowlege_Occupation', 'Show', 'Group', 'Raw_Guest_List'],\n",
       " ['1999', 'actor', '1/11/99', 'Acting', 'Michael J. Fox'],\n",
       " ['1999', 'Comedian', '1/12/99', 'Comedy', 'Sandra Bernhard'],\n",
       " ['1999', 'television actress', '1/13/99', 'Acting', 'Tracey Ullman'],\n",
       " ['1999', 'film actress', '1/14/99', 'Acting', 'Gillian Anderson']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "daily_show.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>One of the wonderful features of PySpark is the ability to separate our logic - which we prefer to write in Python - from the actual data transformation. In the previous code cell, we wrote this lambda function in Python code:</p>\n",
    "</div>\n",
    "<div><textarea style=\"display: none;\">raw_data.map(lambda line: line.split('\\t'))</textarea>\n",
    "\n",
    "<pre>raw_data.map(lambda line: line.split('\\t'))</pre>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<p>Even though the function was in Python, we also took advantage of Scala when Spark actually ran the code over our RDD. <strong>This</strong> is the power of PySpark. Without learning any Scala, we get to harness the data processing performance gains from Spark's Scala architecture. Even better, when we ran the following code, it returned the results to us in Python-friendly notation:</p>\n",
    "</div>\n",
    "<div><textarea style=\"display: none;\">daily_show.take(5)</textarea>\n",
    "\n",
    "<div>\n",
    "<pre>daily_show.take(5)</pre>\n",
    "</div>\n",
    "<div>\n",
    "<p><strong>Transformations and Actions</strong></p>\n",
    "<p>There are two types of methods in Spark:</p>\n",
    "</div>\n",
    "<div><textarea style=\"display: none;\">1. Transformations - map(), reduceByKey()\n",
    "2. Actions - take(), reduce(), saveAsTextFile(), collect()</textarea>\n",
    "\n",
    "<div>\n",
    "<pre>1. Transformations - map(), reduceByKey()</pre>\n",
    "<pre>2. Actions - take(), reduce(), saveAsTextFile(), collect()</pre>\n",
    "    </div>\n",
    "<div>\n",
    "<p>Transformations are lazy operations that always return a reference to an RDD object. Spark doesn't actually run the transformations, though, until an action needs to use the RDD resulting from a transformation. Any function that returns an RDD is a transformation, and any function that returns a value is an action. These concepts will become more clear as we work through this lesson and practice writing PySpark code.</p>\n",
    "<p><strong>Immutability</strong></p>\n",
    "<p>You may be wondering why we couldn't just split each <code>string</code> in place, instead of creating a new object <code>daily_show</code>. In Python, we could have modified the collection element-by-element in place, without returning and assigning the results to a new object.</p>\n",
    "<p>RDD objects are <a href=\"https://www.quora.com/Why-is-a-spark-RDD-immutable\" target=\"_blank\">immutable</a>, meaning that we can't change their values once we've created them. In Python, list and dictionary objects are mutable (we can change their values), while tuple objects are immutable. The only way to modify a tuple object in Python is to create a new tuple object with the necessary updates. Spark uses the immutability of RDDs to enhance calculation speeds. The mechanics of how it does this are outside the scope of this lesson.</p>\n",
    "    \n",
    "We'd like to tally up the number of guests who have appeared on The Daily Show during each year. If daily_show were a list of lists, we could write the following Python code to achieve this result:\n",
    "```python\n",
    "tally = dict()\n",
    "for line in daily_show:\n",
    "  year = line[0]\n",
    "  if year in tally.keys():\n",
    "    tally[year] = tally[year] + 1\n",
    "  else:\n",
    "    tally[year] = 1\n",
    "```\n",
    "The keys in tally will be the years, and the values will be the totals for the number of lines associated with each year.\n",
    "\n",
    "To achieve the same result with Spark, we'll have to use a Map step, then a ReduceByKey step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[11] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "tally = daily_show.map(lambda x: (x[0], 1)).reduceByKey(lambda x,y: x+y)\n",
    "print(tally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that printing tally didn't return the histogram we were hoping for. Because of lazy evaluation, PySpark delayed executing the map and reduceByKey steps until we actually need them. Before we use take() to preview the first few elements in tally, we'll walk through the code we just wrote.\n",
    "\n",
    "daily_show.map(lambda x: (x[0], 1)).reduceByKey(lambda x, y: x+y)\n",
    "During the map step, we used a lambda function to create a tuple consisting of:\n",
    "\n",
    "key: x[0] (the first value in the list)\n",
    "value: 1 (the integer)\n",
    "Our high-level strategy was to create a tuple with the key representing the year, and the value representing 1. After running the map step, Spark will maintain in memory a list of tuples resembling the following:\n",
    "\n",
    "('YEAR', 1)\n",
    "('1991', 1)\n",
    "('1991', 1)\n",
    "('1991', 1)\n",
    "('1991', 1)\n",
    "...\n",
    "We'd like to reduce that down to:\n",
    "\n",
    "('YEAR', 1)\n",
    "('1991', 4)\n",
    "...\n",
    "reduceByKey(f) combines tuples with the same key using the function we specify, f.\n",
    "\n",
    "To see the results of these two steps, we'll use the take command, which forces lazy code to run immediately. Because tally is an RDD, we can't use Python's len function to find out how many elements are in the collection. Instead, we'll need to use the RDD count() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YEAR', 1),\n",
       " ('2002', 159),\n",
       " ('2003', 166),\n",
       " ('2004', 164),\n",
       " ('2007', 141),\n",
       " ('2010', 165),\n",
       " ('2011', 163),\n",
       " ('2012', 164),\n",
       " ('2013', 166),\n",
       " ('2014', 163),\n",
       " ('2015', 100),\n",
       " ('1999', 166),\n",
       " ('2000', 169),\n",
       " ('2001', 157),\n",
       " ('2005', 162),\n",
       " ('2006', 161),\n",
       " ('2008', 164),\n",
       " ('2009', 163)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tally.take(tally.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about column headers, and didn't set them aside. We need a way to remove the element ('YEAR', 1) from our collection. We'll need a workaround, though, because RDD objects are immutable once we create them. The only way to remove that tuple is to create a new RDD object that doesn't have it.\n",
    "\n",
    "Spark comes with a filter(f) function that creates a new RDD by filtering an existing one for specific criteria. If we specify a function f that returns a binary value, True or False, the resulting RDD will consist of elements where the function evaluated to True. You can read more about the filter function in the Spark documentation.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Write a function named filter_year that we can use to filter out the element that begins with the text YEAR, instead of an actual year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_year(line):\n",
    "    if line[0].startswith('YEAR'):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "filtered_daily_show = daily_show.filter(lambda line: filter_year(line))\n",
    "#filtered_daily_show.take(filtered_daily_show.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actor', 596),\n",
       " ('film actress', 21),\n",
       " ('model', 9),\n",
       " ('stand-up comedian', 44),\n",
       " ('actress', 271)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how spark is better than mapreduce????!!\n",
    "filtered_daily_show.filter(lambda line: line[1] != '') \\\n",
    "                   .map(lambda line: (line[1].lower(), 1)) \\\n",
    "                   .reduceByKey(lambda x,y: x+y) \\\n",
    "                   .take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
