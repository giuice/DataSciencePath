{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "Before we can write and run SQL queries, we need to tell Spark to treat the DataFrame as a SQL table. Spark internally maintains a virtual database within the SQLContext object. This object, which we enter as sqlCtx, has methods for registering temporary tables.\n",
    "\n",
    "To register a DataFrame as a table, call the registerTempTable() method on that DataFrame object. This method requires one string parameter, name, that we use to set the table name for reference in our SQL queries.\n",
    "\n",
    "1. Use the registerTempTable() method to register the DataFrame df as a table named census2010.\n",
    "    - Then, run the SQLContext method tableNames to return the list of tables.\n",
    "2. Assign the resulting list to tables, and use the print function to display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "# Import PySpark and initialize SparkContext object.\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "df = sqlCtx.read.json(\"data/census_2010.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['census2010']\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('census2010')\n",
    "tables = sqlCtx.tableNames()\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've registered the table within sqlCtx, we can start writing and running SQL queries. With Spark SQL, we represent our query as a string and pass it into the sql() method within the SQLContext object. The sql() method requires a single parameter, the query string. Spark will return the query results as a DataFrame object. This means you'll have to use show() to display the results, due to lazy loading.\n",
    "\n",
    "While SQLite requires that queries end with a semi-colon, Spark SQL will actually throw an error if you include it. Other than this difference in syntax, Spark's flavor of SQL is identical to SQLite, and all the queries you've written for the SQL course will work here as well.\n",
    "1. Write a SQL query that returns the age column from the table census2010, and use the show() method to display the first 20 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"select age from census2010\"\n",
    "sqlCtx.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL shines at expressing complex logic in a more compact manner. Let's brush up on SQL by writing a query that expresses more specific criteria.\n",
    "\n",
    "\n",
    "1. Write and run a SQL query that returns:\n",
    "2. The males and females columns (in that order) where age > 5 and age < 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|             males|           females|\n",
      "+-------+------------------+------------------+\n",
      "|  count|                 9|                 9|\n",
      "|   mean|         2124558.0|2031901.5555555555|\n",
      "| stddev|33572.118476497715|26969.739459768203|\n",
      "|    min|           2093905|           2007781|\n",
      "|    max|           2195773|           2089651|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'select males, females from census2010 where age > 5 and age <15'\n",
    "sqlCtx.sql(query).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most powerful use cases in SQL is joining tables. Spark SQL takes this a step further by enabling you to run join queries across data from multiple file types. Spark will read any of the file types and formats it supports into DataFrame objects and we can register each of these as tables within the SQLContext object to use for querying.\n",
    "\n",
    "As we mentioned briefly in the previous mission, most data science organizations use a variety of file formats and data storage mechanisms. Spark SQL was built with the industry use cases in mind and enables data professionals to use one common query language, SQL, to interact with lots of different data sources. We'll explore joins in Spark SQL further, but first let's introduce the other datasets we'll be using:\n",
    "<pre>\n",
    "census_1980.json - 1980 U.S. Census data\n",
    "census_1990.json - 1990 U.S. Census data\n",
    "census_2000.json - 2000 U.S. Census data\n",
    "</pre>\n",
    "1. Read these additional datasets into DataFrame objects and then use the registerTempTable() function to register these tables individually within SQLContext:\n",
    "    - census_1980.json as census1980,\n",
    "    - census_1990.json as census1990,\n",
    "    - census_2000.json as census2000.\n",
    "2. Then use the method tableNames() to list the tables within the SQLContext object, assign to tables, and finally print tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['census1980', 'census1990', 'census2000', 'census2010']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1980 = sqlCtx.read.json(\"data/census_1980.json\")\n",
    "df1990 = sqlCtx.read.json(\"data/census_1990.json\")\n",
    "df2000 = sqlCtx.read.json(\"data/census_2000.json\")\n",
    "df1980.registerTempTable('census1980')\n",
    "df1990.registerTempTable('census1990')\n",
    "df2000.registerTempTable('census2000')\n",
    "sqlCtx.tableNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a query that returns a DataFrame with the total columns for the tables census2010 and census2000 (in that order).\n",
    "2. Then, run the query and use the show() method to display the first 20 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(total=4079669, total=3733034), Row(total=4085341, total=3825896)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"select a.total, b.total from census2010 a join census2000 b on a.age = b.age\"\n",
    "sqlCtx.sql(query).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions and operators from SQLite that we've used in the past are available for us to use in Spark SQL:\n",
    "\n",
    "- COUNT()\n",
    "- AVG()\n",
    "- SUM()\n",
    "- AND\n",
    "- OR\n",
    "\n",
    "1. Write a query that calculates the sums of the total column from each of the tables, in the following order:\n",
    "<pre>\n",
    "census2010,\n",
    "census2000,\n",
    "census1990.\n",
    "</pre>\n",
    "You'll need to perform two inner joins for this query (all datasets have the same values for age, which makes things convenient for joining)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(total2010=312247116, total2000=284594395, total1990=254506647)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"select sum(a.total) as total2010, sum(b.total) as total2000, sum(c.total) as total1990 from census2010 a  join \\\n",
    "census2000 b on a.age = b.age join census1990 c on b.age = c.age\"\n",
    "sqlCtx.sql(query).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
