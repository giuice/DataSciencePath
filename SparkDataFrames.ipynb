{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrames\n",
    "\n",
    "The Spark DataFrame is a feature that allows you to create and work with DataFrame objects. As you may have guessed, pandas inspired it.\n",
    "\n",
    "Spark is well known for its ability to process large data sets. Spark DataFrames combine the scale and speed of Spark with the familiar query, filter, and analysis capabilities of pandas. Unlike pandas, which can only run on one computer, Spark can use distributed memory (and disk when necessary) to handle larger data sets and run computations more quickly.\n",
    "\n",
    "Spark DataFrames allow us to modify and reuse our existing pandas code to scale up to much larger data sets. They also have better support for various data formats. We can even use a SQL interface to write distributed SQL queries that query large database systems and other data stores.\n",
    "\n",
    "For this mission, we'll be working with a JSON file containing data from the 2010 U.S. Census. It has the following columns:\n",
    "\n",
    "- age - Age (year)\n",
    "- females - Number of females\n",
    "- males - Number of males\n",
    "- total - Total number of individuals\n",
    "- year - Year column (2010 for all rows)\n",
    "\n",
    "Let's open and explore the data set before we dive into Spark DataFrames.\n",
    "\n",
    "Instructions\n",
    "\n",
    " 1. Print the first four lines of census_2010.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find path to PySpark.\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import PySpark and initialize SparkContext object.\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"females\": 1994141, \"total\": 4079669, \"males\": 2085528, \"age\": 0, \"year\": 2010}',\n",
       " '{\"females\": 1997991, \"total\": 4085341, \"males\": 2087350, \"age\": 1, \"year\": 2010}',\n",
       " '{\"females\": 2000746, \"total\": 4089295, \"males\": 2088549, \"age\": 2, \"year\": 2010}',\n",
       " '{\"females\": 2002756, \"total\": 4092221, \"males\": 2089465, \"age\": 3, \"year\": 2010}']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile('data/census_2010.json')\n",
    "rdd.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous missions, we explored reading data into an RDD object. Recall that an RDD is essentially a list of tuples with no enforced schema or structure of any kind. An RDD can have a variable number of elements in each tuple, and combinations of types between tuples.\n",
    "\n",
    "RDDs are useful for representing unstructured data like text. Without them, we'd need to write a lot of custom Python code to interact with such data.\n",
    "\n",
    "We use the SparkContext object to read data into an RDD:\n",
    "<pre>\n",
    "raw_data = sc.textFile(\\\"daily_show.tsv\\\")\n",
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "</pre>\n",
    "To use the familar DataFrame query interface from pandas, however, the data representation needs to include rows, columns, and types. Spark's implementation of DataFrames mirrors the pandas implementation, with logic for rows and columns.\n",
    "\n",
    "The Spark SQL class is very powerful. It gives Spark more information about the data structure we're using and the computations we want to perform. Spark uses that information to optimize processes.\n",
    "\n",
    "To take advantage of these features, we'll have to use the SQLContext object to structure external data as a DataFrame, instead of the SparkContext object.\n",
    "\n",
    "We can query Spark DataFrame objects with SQL, which we'll explore in the next mission. The SQLContext class gets its name from this capability.\n",
    "\n",
    "This class allows us to read in data and create new DataFrames from a wide range of sources. It can do this because it takes advantage of Spark's powerful Data Sources API.\n",
    "\n",
    "File Formats\n",
    "\n",
    "- JSON, CSV/TSV, XML\n",
    "- Parquet, Amazon S3 (cloud storage service)\n",
    "\n",
    "Big Data Systems\n",
    "\n",
    "- Hive, Avro, HBase\n",
    "\n",
    "SQL Database Systems\n",
    "- MySQL, PostgreSQL\n",
    "\n",
    "Data science organizations often use a wide range of systems to collect and store data, and they're constantly making changes to those systems. Spark DataFrames allow us to interface with different types of data, and ensure that our analysis logic will still work as the data storage mechanisms change.\n",
    "\n",
    "Now that you've learned a bit about Spark DataFrames, let's read in census_2010.json. This data set contains valid JSON on each line, which is what Spark needs in order to read the data in properly.\n",
    "\n",
    "In the following code cell, we:\n",
    "\n",
    "1. Import SQLContext from the pyspark.sql library\n",
    "2. Instantiate the SQLContext object (which requires the SparkContext object (sc) as a parameter), and assign it to the variable sqlCtx\n",
    "3. Use the SQLContext method read.json() to read the JSON data set into a Spark DataFrame object named df\n",
    "4. Print df's data type to confirm that we successfully read it in as a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Import SQLContext\n",
    "from pyspark.sql import SQLContext\n",
    "# Pass in the SparkContext object `sc`\n",
    "sqlCtx = SQLContext(sc)\n",
    "# Read JSON data into a DataFrame object `df`\n",
    "df = sqlCtx.read.json(\"data/census_2010.json\")\n",
    "# Print the type\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we read data into the SQLContext object, Spark:\n",
    "\n",
    "Instantiates a Spark DataFrame object\n",
    "Infers the schema from the data and associates it with the DataFrame\n",
    "Reads in the data and distributes it across clusters (if multiple clusters are available)\n",
    "Returns the DataFrame object\n",
    "We expect the DataFrame Spark created to have the following columns, which were the keys in the JSON data set:\n",
    "<pre>\n",
    "age\n",
    "females\n",
    "males\n",
    "total\n",
    "year\n",
    "</pre>\n",
    "Spark has its own type system that's similar to the pandas type system. To create a DataFrame, Spark iterates over the data set twice - once to extract the structure of the columns, and once to infer each column's type. Let's use one of the Spark DataFrame instance methods to display the schema for the DataFrame we're working with.\n",
    "1. Call the printSchema() method on the Spark DataFrame df to display the schema that Spark inferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- females: long (nullable = true)\n",
      " |-- males: long (nullable = true)\n",
      " |-- total: long (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlCtx = SQLContext(sc)\n",
    "df = sqlCtx.read.json(\"data/census_2010.json\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned before, the pandas DataFrame heavily influenced the Spark DataFrame implementation. Here are some of the methods we can find in both:\n",
    "<pre style='color:green'>\n",
    "agg()\n",
    "join()\n",
    "sort()\n",
    "where()\n",
    "</pre>\n",
    "\n",
    "Unlike pandas DataFrames, however, Spark DataFrames are immutable, which means we can't modify existing objects. Most transformations on an object return a new DataFrame reflecting the changes instead. As we discussed in previous missions, Spark's creators deliberately designed immutability into Spark to make it easier to work with distributed data structures.\n",
    "\n",
    "Pandas and Spark DataFrames also have different underlying data structures. Pandas DataFrames are built around Series objects, while Spark DataFrames are built around RDDs. We can perform most of the same computations and transformations on Spark DataFrames that we can on pandas DataFrames, but the styles and methods are somewhat different. We'll explore how to perform common pandas functions with Spark in this mission.\n",
    "\n",
    "Instructions\n",
    "\n",
    "1. Use the show() method to print the first five rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+-------+----+\n",
      "|age|females|  males|  total|year|\n",
      "+---+-------+-------+-------+----+\n",
      "|  0|1994141|2085528|4079669|2010|\n",
      "|  1|1997991|2087350|4085341|2010|\n",
      "|  2|2000746|2088549|4089295|2010|\n",
      "|  3|2002756|2089465|4092221|2010|\n",
      "|  4|2004366|2090436|4094802|2010|\n",
      "+---+-------+-------+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas, we used the head() method to return the first n rows. This is one of the differences between the DataFrame implementations. Instead of returning a nicely formatted table of values, the head() method in Spark returns a list of row objects. Spark needs to return row objects for certain methods, such as head(), collect() and take().\n",
    "\n",
    "You can access a row's attributes by the column name using dot notation, and by position using bracket notation with an index:\n",
    "<pre style='color:green'>\n",
    "row_one = df.head(5)[0]\n",
    "# Access value for age\n",
    "row_one.age\n",
    "# Access the first value\n",
    "row_one[0]\n",
    "</pre>\n",
    "Instructions\n",
    "\n",
    "1. Use the head() method to return the first five rows in the DataFrame as a list of row objects, and assign the result to the variable first_five.\n",
    "2. Print the age value for each row object in first_five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "first_five = df.head(5)\n",
    "for obj in first_five:\n",
    "    print(obj.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas, we passed a string into a single pair of brackets ([]) to select an individual column, and passed in a list to select multiple columns:\n",
    "<pre>\n",
    "# Pandas DataFrame\n",
    "df['age']\n",
    "df[['age', 'males']]\n",
    "</pre>\n",
    "We can still use bracket notation in Spark. We'll need to pass in a list of string objects, though, even when we're only selecting one column.\n",
    "\n",
    "Spark takes advantage of lazy loading with DataFrames, and will only display the results of an operation when we call the show() method. Instead of using bracket notation, we can also use the select() method to select columns:\n",
    "<pre>\n",
    "# Spark DataFrame\n",
    "df.select('age')\n",
    "df.select('age', 'males')\n",
    "</pre>\n",
    "In the following code cell, we demonstrate how to select and display the age column. Use what you've learned to take this a step farther and select multiple columns.\n",
    "\n",
    "Instructions\n",
    "\n",
    "1. Select the age, males, and females columns from the DataFrame and display them using the show() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|  males|\n",
      "+---+-------+\n",
      "|  0|2085528|\n",
      "|  1|2087350|\n",
      "|  2|2088549|\n",
      "|  3|2089465|\n",
      "|  4|2090436|\n",
      "+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df[['age']].show()\n",
    "df.select('age','males').show(5) # or df[['age','males',...]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas, we used Boolean filtering to select only the rows we were interested in. Spark preserves the very same functionality and notation.\n",
    "\n",
    "Instructions\n",
    "\n",
    "1. Use the pandas notation for Boolean filtering to select the rows where age is greater than five.\n",
    "2. Assign the resulting DataFrame to the variable five_plus.\n",
    "3. Use the show() method to display five_plus.\n",
    "## Duas maneiras de se fazer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+-------+----+\n",
      "|age|females|  males|  total|year|\n",
      "+---+-------+-------+-------+----+\n",
      "|  6|2007781|2093905|4101686|2010|\n",
      "|  7|2010281|2097080|4107361|2010|\n",
      "|  8|2013771|2101670|4115441|2010|\n",
      "|  9|2018603|2108014|4126617|2010|\n",
      "| 10|2023289|2114217|4137506|2010|\n",
      "+---+-------+-------+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Age > 5\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+-------+----+\n",
      "|age|females|  males|  total|year|\n",
      "+---+-------+-------+-------+----+\n",
      "|  6|2007781|2093905|4101686|2010|\n",
      "|  7|2010281|2097080|4107361|2010|\n",
      "|  8|2013771|2101670|4115441|2010|\n",
      "|  9|2018603|2108014|4126617|2010|\n",
      "| 10|2023289|2114217|4137506|2010|\n",
      "+---+-------+-------+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "five_plus = df[df['age'] > 5]\n",
    "five_plus.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the columns in Spark DataFrames with each other, and use the comparison criteria as a filter. For example, to get the rows where the population of males execeeded females in 2010, we'd write the same notation that we would use in pandas.\n",
    "\n",
    "Instructions\n",
    "\n",
    "1. Find all of the rows where females is less than males, and use show() to display the first 20 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+-------+----+\n",
      "|age|females|  males|  total|year|\n",
      "+---+-------+-------+-------+----+\n",
      "|  0|1994141|2085528|4079669|2010|\n",
      "|  1|1997991|2087350|4085341|2010|\n",
      "|  2|2000746|2088549|4089295|2010|\n",
      "|  3|2002756|2089465|4092221|2010|\n",
      "|  4|2004366|2090436|4094802|2010|\n",
      "+---+-------+-------+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[df['females'] < df['males']].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
