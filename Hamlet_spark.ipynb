{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hamelet and Spark!\n",
    "\n",
    "In a previous lesson, we touched briefly on transformations and actions, and how these two methods affect the execution of code. In this lesson, we'll dive deeper into how those mechanisms work, and explore a wider range of the functions built into the Spark core.\n",
    "\n",
    "The file hamlet.txt contains the entire text of Shakespeare's play Hamlet. Shakespeare is well-known for his unique writing style and arguably one of the most influential writers in history. Hamlet is one of his most popular plays.\n",
    "\n",
    "Let's perform some text analysis on it. The file is in pure text format, though, and not ready for analysis. Before we can proceed, we'll have to clean up and reformat the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find path to PySpark.\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import PySpark and initialize SparkContext object.\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hamlet@0\\t\\tHAMLET',\n",
       " 'hamlet@8',\n",
       " 'hamlet@9',\n",
       " 'hamlet@10\\t\\tDRAMATIS PERSONAE',\n",
       " 'hamlet@29']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_hamlet = sc.textFile('data/hamlet.txt')\n",
    "raw_hamlet.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text file uses the tab character (\\t) as a delimiter. We'll need to split the file on the tab delimiter and convert the results into an RDD that's more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_hamlet = raw_hamlet.map(lambda line: line.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hamlet@0', '', 'HAMLET'],\n",
       " ['hamlet@8'],\n",
       " ['hamlet@9'],\n",
       " ['hamlet@10', '', 'DRAMATIS PERSONAE'],\n",
       " ['hamlet@29'],\n",
       " ['hamlet@30'],\n",
       " ['hamlet@31', 'CLAUDIUS', 'king of Denmark. (KING CLAUDIUS:)'],\n",
       " ['hamlet@74'],\n",
       " ['hamlet@75', 'HAMLET', 'son to the late, and nephew to the present king.'],\n",
       " ['hamlet@131'],\n",
       " ['hamlet@132', 'POLONIUS', 'lord chamberlain. (LORD POLONIUS:)'],\n",
       " ['hamlet@176'],\n",
       " ['hamlet@177', 'HORATIO', 'friend to Hamlet.'],\n",
       " ['hamlet@203'],\n",
       " ['hamlet@204', 'LAERTES', 'son to Polonius.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_hamlet.take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda functions are great for writing quick functions we can pass into PySpark methods with simple logic. They fall short when we need to write more customized logic, though. Thankfully, PySpark lets us define a function in Python first, then pass it in. Any function that returns a sequence of data in PySpark (versus a guaranteed Boolean value, like filter() requires) must use a yield statement to specify the values that should be pulled later.\n",
    "\n",
    "If you're unfamiliar with the yield statement in Python, read this excellent Stack Overflow answer on the topic. To summarize, yield is a Python technique that allows the interpreter to generate data on the fly and pull it when necessary, instead of storing it to memory immediately. Because of its unique architecture, Spark takes advantage of this technique to reduce overhead and improve the speed of computations.\n",
    "\n",
    "Spark runs the named function on every element in the RDD and restricts it in scope. Each instance of the function only has access to the object(s) you pass into the function, and the Python libraries available in your environment. If you try to refer to variables outside the scope of the function or import libraries, those actions may cause the computation to crash. That's because Spark compiles the function's code to Java to run on the RDD objects (which are also in Java).\n",
    "\n",
    "Finally, not all functions require us to use yield; only the ones that generate a custom sequence of data do. For map() or filter(), we use return to return a value for every single element in the RDD we're running the functions on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code cell, we'll use the flatMap() method with the named function hamlet_speaks to check whether a line in the play contains the text HAMLET in all caps (indicating that Hamlet spoke). flatMap() is different than map() because it doesn't require an output for every element in the RDD. The flatMap() method is useful whenever we want to generate a sequence of values from an RDD.\n",
    "\n",
    "In this case, we want an RDD object that contains tuples of the unique line IDs and the text \"hamlet speaketh!,\" but only for the elements in the RDD that have \"HAMLET\" in one of the values. We can't use the map() method for this because it requires a return value for every element in the RDD.\n",
    "\n",
    "We want each element in the resulting RDD to have the following format:\n",
    "\n",
    "1. The first value should be the unique line ID (e.g.'hamlet@0') , which is the first value in each of the elements in the split_hamlet RDD.\n",
    "\n",
    "2. The second value should be the string \"hamlet speaketh!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hamlet@0', 'hamlet speaketh!'),\n",
       " ('hamlet@75', 'hamlet speaketh!'),\n",
       " ('hamlet@1004', 'hamlet speaketh!'),\n",
       " ('hamlet@9144', 'hamlet speaketh!'),\n",
       " ('hamlet@12313', 'hamlet speaketh!'),\n",
       " ('hamlet@12434', 'hamlet speaketh!'),\n",
       " ('hamlet@12760', 'hamlet speaketh!'),\n",
       " ('hamlet@12858', 'hamlet speaketh!'),\n",
       " ('hamlet@14821', 'hamlet speaketh!'),\n",
       " ('hamlet@15261', 'hamlet speaketh!')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hamlet_speaks(line):\n",
    "    id = line[0]\n",
    "    speaketh = False\n",
    "    \n",
    "    if \"HAMLET\" in line:\n",
    "        speaketh = True\n",
    "    \n",
    "    if speaketh:\n",
    "        yield id,\"hamlet speaketh!\"\n",
    "\n",
    "hamlet_spoken = split_hamlet.flatMap(lambda x: hamlet_speaks(x))\n",
    "hamlet_spoken.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hamlet_spoken now contains the line numbers for the lines where Hamlet spoke. While this is handy, we don't have the full line anymore. Instead, let's use a filter() with a named function to extract the original lines where Hamlet spoke. The functions we pass into filter() must return values, which will be either True or False.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Write a named function filter_hamlet_speaks to pass into filter(). Apply it to split_hamlet to return an RDD with the elements containing the word HAMLET.\n",
    "2. Assign the resulting RDD to hamlet_spoken_lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hamlet@0', '', 'HAMLET'],\n",
       " ['hamlet@75', 'HAMLET', 'son to the late, and nephew to the present king.'],\n",
       " ['hamlet@1004', '', 'HAMLET'],\n",
       " ['hamlet@9144', '', 'HAMLET'],\n",
       " ['hamlet@12313',\n",
       "  'HAMLET',\n",
       "  '[Aside]  A little more than kin, and less than kind.']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_hamlet_speaks(line):\n",
    "    if \"HAMLET\" in line:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "hamlet_spoken_lines = split_hamlet.filter(lambda line: filter_hamlet_speaks(line))\n",
    "hamlet_spoken_lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've discussed before, Spark has two kinds of methods, transformations and actions. While we've explored some of the transformations, we haven't used any actions other than take().\n",
    "\n",
    "Whenever we use an action method, Spark forces the evaluation of lazy code. If we only chain together transformation methods and print the resulting RDD object, we'll see the type of RDD (e.g. a PythonRDD or PipelinedRDD object), but not the elements within it. That's because the computation hasn't actually happened yet.\n",
    "\n",
    "Even though Spark simplifies chaining lots of transformations together, it's good practice to use actions to observe the intermediate RDD objects between those transformations. This will let you know whether your transformations are working the way you expect them to.\n",
    "\n",
    "#### Count()\n",
    "\n",
    "The count() method returns the number of elements in an RDD. count() is useful when we want to make sure the result of a transformation contains the right number of elements. For example, if we know there should be an element in the resulting RDD for every element in the initial RDD, we can compare the counts of both to ensure they match.\n",
    "\n",
    "To get the number of elements in the RDD hamlet_spoken_lines, run .count() on it:\n",
    "\n",
    "hamlet_spoken_lines.count()\n",
    "\n",
    "#### Collect()\n",
    "\n",
    "We've used take() to preview the first few elements of an RDD, similar to the way we've use head() in pandas. But what about returning all of the elements in a collection? We need to do this to write an RDD to a CSV, for example. It's also useful for running some basic Python code over a collection without going through PySpark.\n",
    "\n",
    "Running .collect() on an RDD returns a list representation of it. To get a list of all the elements in hamlet_spoken_lines, for example, we would write:\n",
    "\n",
    "hamlet_spoken_lines.collect()\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "1. Compute the number of elements in hamlet_spoken_lines, and assign the result to the variable named spoken_count.\n",
    "2. Grab the 101st element in hamlet_spoken_lines (which has the list index 100), and assign that list to spoken_101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoken_count = 0\n",
    "spoken_101 = list()\n",
    "spoken_count = hamlet_spoken_lines.count()\n",
    "spoken_101 = hamlet_spoken_lines.collect()[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first value in each element (or line from the play) is a line number that identifies the line of the play the text is from. It appears in the following format:\n",
    "\n",
    "'hamlet@0'\n",
    "'hamlet@8',\n",
    "'hamlet@9',\n",
    "...\n",
    "We don't need the hamlet@ at the beginning of these IDs for our data analysis. Let's extract just the integer part of the ID from each line, which is much more useful.\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "1. Transform the RDD split_hamlet into a new RDD hamlet_with_ids that contains the clean version of the line ID for each element.\n",
    "\n",
    "- For example, we want to transform hamlet@0 to 0, and leave the rest of the values in that element untouched.\n",
    "    - Recall that the map() function will run on each element in the RDD, where each element is a list that we can access using regular Python mechanics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hamlet@0', '', 'HAMLET'],\n",
       " ['hamlet@8'],\n",
       " ['hamlet@9'],\n",
       " ['hamlet@10', '', 'DRAMATIS PERSONAE'],\n",
       " ['hamlet@29']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_hamlet = raw_hamlet.map(lambda line: line.split('\\t'))\n",
    "split_hamlet.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', '', 'HAMLET'],\n",
       " ['8'],\n",
       " ['9'],\n",
       " ['10', '', 'DRAMATIS PERSONAE'],\n",
       " ['29'],\n",
       " ['30'],\n",
       " ['31', 'CLAUDIUS', 'king of Denmark. (KING CLAUDIUS:)'],\n",
       " ['74'],\n",
       " ['75', 'HAMLET', 'son to the late, and nephew to the present king.'],\n",
       " ['131']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_line(x):\n",
    "    x[0] = x[0].split('hamlet@')[1]\n",
    "    return x\n",
    "\n",
    "hamlet_with_ids = split_hamlet.map(map_line)\n",
    "hamlet_with_ids.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to get rid of elements that don't contain any actual words (and just have an ID as the first value). These typically represent blank lines between paragraphs or sections in the play. We also want to remove any blank values ('') within elements, which don't contain any useful information for our analysis.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Clean up the RDD and store the result as a new RDD hamlet_text_only.lalasldkfjsadflkjqwerillkwjaçsdflkjwoierqkj asdfk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', 'HAMLET'],\n",
       " ['10', 'DRAMATIS PERSONAE'],\n",
       " ['31', 'CLAUDIUS', 'king of Denmark. (KING CLAUDIUS:)'],\n",
       " ['75', 'HAMLET', 'son to the late, and nephew to the present king.'],\n",
       " ['132', 'POLONIUS', 'lord chamberlain. (LORD POLONIUS:)']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def removeBlank(line):\n",
    "        return list(filter(None,line))\n",
    "    \n",
    "hamlet_text_only = hamlet_with_ids.filter(lambda x: True if len(x) > 1 else False).map(removeBlank)\n",
    "hamlet_text_only.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['|', 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', 'HAMLET'],\n",
       " ['10', 'DRAMATIS PERSONAE'],\n",
       " ['31', 'CLAUDIUS', 'king of Denmark. (KING CLAUDIUS:)'],\n",
       " ['75', 'HAMLET', 'son to the late, and nephew to the present king.'],\n",
       " ['132', 'POLONIUS', 'lord chamberlain. (LORD POLONIUS:)'],\n",
       " ['177', 'HORATIO', 'friend to Hamlet.'],\n",
       " ['204', 'LAERTES', 'son to Polonius.'],\n",
       " ['230', 'LUCIANUS', 'nephew to the king.'],\n",
       " ['261', 'VOLTIMAND'],\n",
       " ['273']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet_text_only.map(lambda x: [y for y in x if y != '|']).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4376"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fix_pipe(line):\n",
    "    results = list()\n",
    "    for l in line:\n",
    "        if l == \"|\":\n",
    "            pass\n",
    "        elif \"|\" in l:\n",
    "            fmtd = l.replace(\"|\", \"\")\n",
    "            results.append(fmtd)\n",
    "        else:\n",
    "            results.append(l)\n",
    "    return results\n",
    "\n",
    "hamlet_text_only.map(lambda line: fix_pipe(line)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
